{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in d:\\programs\\anaconda\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (1.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (1.21.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (3.19.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (1.42.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (2.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard) (61.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\programs\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programs\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in d:\\programs\\anaconda\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: torch==1.13.0 in d:\\programs\\anaconda\\lib\\site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: numpy in d:\\programs\\anaconda\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programs\\anaconda\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in d:\\programs\\anaconda\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\programs\\anaconda\\lib\\site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programs\\anaconda\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# sklearn classes\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# sklearn utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encoders\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0    male        group A                 high school      standard   \n",
       "1  female        group D            some high school  free/reduced   \n",
       "2    male        group E                some college  free/reduced   \n",
       "3    male        group B                 high school      standard   \n",
       "4    male        group E          associate's degree      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0               completed          67             67             63  \n",
       "1                    none          40             59             55  \n",
       "2                    none          59             60             50  \n",
       "3                    none          77             78             68  \n",
       "4               completed          78             73             68  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Task 1.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   gender                       1000 non-null   object\n",
      " 1   race/ethnicity               1000 non-null   object\n",
      " 2   parental level of education  1000 non-null   object\n",
      " 3   lunch                        1000 non-null   object\n",
      " 4   test preparation course      1000 non-null   object\n",
      " 5   math score                   1000 non-null   int64 \n",
      " 6   reading score                1000 non-null   int64 \n",
      " 7   writing score                1000 non-null   int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keys for gender: ['male' 'female']\n",
      "Unique keys for race/ethnicity: ['group A' 'group D' 'group E' 'group B' 'group C']\n",
      "Unique keys for parental level of education: ['high school' 'some high school' 'some college' \"associate's degree\"\n",
      " \"bachelor's degree\" \"master's degree\"]\n",
      "Unique keys for lunch: ['standard' 'free/reduced']\n",
      "Unique keys for test preparation course: ['completed' 'none']\n",
      "Unique keys for math score: [ 67  40  59  77  78  63  62  93  47  99  80  74  81  69  58  54  23  39\n",
      "  83  71  44  46  51  53  33  87  65  79  68  57  98 100  60  64  52  36\n",
      "  56  88  42  76  49  72  31  32  97  94  95  45  75  66  70  82  92  30\n",
      "  41  43  55  73  48  86  84  50  89  91  85  61  35  90  96  13  38  28\n",
      "  26  37  29  34  25]\n",
      "Unique keys for reading score: [ 67  59  60  78  73  77  88  56  42  83  87  74  61  47  62  44  32  76\n",
      "  52  69  55  38  45  68  89  54  79  41  65  81  75  70  46  63  72  58\n",
      "  53  57  80  51  66  82  93  64  91  43  71 100  50  33  34  90  84  95\n",
      "  98  92  85  49  86  48  94  36  28  39  29  96  97  40  27  37  99  31\n",
      "  35]\n",
      "Unique keys for writing score: [ 63  55  50  68  76  84  65  45  85  90  73  57  42  44  31  88  54  32\n",
      "  56  60  89  51  77  39  71  74  75  72  64  82  70  87  78  49  47  62\n",
      "  83  48  59  97  81  67  69  61  93 100  53  79  58  33  86  66  46  80\n",
      "  91  92  95  99  96  28  52  24  40  43  94  23  38  30  35  41  98  36\n",
      "  27  26  34  37]\n"
     ]
    }
   ],
   "source": [
    "for key in dataset.keys():\n",
    "    print(\"Unique keys for {}: {}\".format(key, dataset[key].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X, y = dataset.drop(columns=['math score', 'writing score', 'reading score']), dataset[['math score', 'writing score', 'reading score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0    male        group A                 high school      standard   \n",
       "1  female        group D            some high school  free/reduced   \n",
       "2    male        group E                some college  free/reduced   \n",
       "3    male        group B                 high school      standard   \n",
       "4    male        group E          associate's degree      standard   \n",
       "\n",
       "  test preparation course  \n",
       "0               completed  \n",
       "1                    none  \n",
       "2                    none  \n",
       "3                    none  \n",
       "4               completed  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>reading score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>68</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   math score  writing score  reading score\n",
       "0          67             63             67\n",
       "1          40             55             59\n",
       "2          59             50             60\n",
       "3          77             68             78\n",
       "4          78             68             73"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "num_features = len(pre_X.keys())\n",
    "num_outputs = len(y.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_column_transformer = ColumnTransformer([\n",
    "    ('ordinal', OrdinalEncoder(), pre_X.keys()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_column_transformer.fit_transform(pre_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  1  1  1  1  1\n",
       "1  2  2  2  2  2\n",
       "2  1  3  3  2  2\n",
       "3  1  4  1  1  2\n",
       "4  1  3  4  1  1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>reading score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   math score  writing score  reading score\n",
       "0        0.67           0.63           0.67\n",
       "1        0.40           0.55           0.59\n",
       "2        0.59           0.50           0.60\n",
       "3        0.77           0.68           0.78\n",
       "4        0.78           0.68           0.73"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "  '''\n",
    "  Prepare the Boston dataset for regression\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X, y, scale_data=False):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      # Apply scaling if necessary\n",
    "      if scale_data:\n",
    "          X = StandardScaler().fit_transform(X)\n",
    "      self.X = torch.from_numpy(X)\n",
    "      self.y = torch.from_numpy(y)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return self.X[i].float(), self.y[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = MyDataset(X_train.to_numpy(dtype='float64'), y_train.to_numpy(dtype='float64'))\n",
    "testdataset = MyDataset(X_test.to_numpy(dtype='float64'), y_test.to_numpy(dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traindataset, batch_size=train_batch_size, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testdataset, batch_size=test_batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:132: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GT 710 which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: d:\\Programs\\Anaconda\n",
      "\n",
      "  added / updated specs:\n",
      "    - cpuonly\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cpuonly-2.0                |                0           2 KB  pytorch\n",
      "    torchaudio-0.13.0          |         py39_cpu         4.5 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cpuonly            pytorch/noarch::cpuonly-2.0-0 None\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  pytorch                    1.13.0-py3.9_cuda11.6_cudnn8_0 --> 1.13.0-py3.9_cpu_0 None\n",
      "  pytorch-mutex                                    1.0-cuda --> 1.0-cpu None\n",
      "  torchaudio                              0.13.0-py39_cu116 --> 0.13.0-py39_cpu None\n",
      "  torchvision                             0.14.0-py39_cu116 --> 0.14.0-py39_cpu None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "cpuonly-2.0          | 2 KB      |            |   0% \n",
      "cpuonly-2.0          | 2 KB      | ########## | 100% \n",
      "cpuonly-2.0          | 2 KB      | ########## | 100% \n",
      "\n",
      "torchaudio-0.13.0    | 4.5 MB    |            |   0% \n",
      "torchaudio-0.13.0    | 4.5 MB    | 2          |   2% \n",
      "torchaudio-0.13.0    | 4.5 MB    | ##8        |  28% \n",
      "torchaudio-0.13.0    | 4.5 MB    | ######4    |  65% \n",
      "torchaudio-0.13.0    | 4.5 MB    | ########## | 100% \n",
      "torchaudio-0.13.0    | 4.5 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import M\n",
    "\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate ANN\n",
    "input_dim = num_features\n",
    "\n",
    "hidden_dim = 300\n",
    "output_dim = num_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "def train( model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start_time = time()\n",
    "    correct = 0\n",
    "    log_interval = 3\n",
    "\n",
    "    mses = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(target[0:1])\n",
    "        output = model(data)\n",
    "        #print(output[0:1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        temp_mse = mean_squared_error(target, output.clone().detach())\n",
    "        mses.append(temp_mse)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'\\rTrain Epoch {epoch}:',\n",
    "                  f'[{batch_idx * len(data)}/{len(train_loader.dataset)}',\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]',\n",
    "                  f'\\tLoss: {loss.item():.6f}',\n",
    "                  end='')\n",
    "\n",
    "    mse = np.mean(mses)\n",
    "    print(f'\\rTrain Epoch: {epoch} Average Loss: {epoch_loss/len(train_loader.dataset):.6f}, elapsed time:{time()-start_time:.2f}s')\n",
    "    return epoch_loss, float(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "    \n",
    "    mse = mean_squared_error(target, output)\n",
    "    print(f'\\rTest set: Average loss: {test_loss/len(test_loader.dataset):.4f},',\n",
    "          f'MSE: {mse:.4f}', \n",
    "          f'\\n')\n",
    "    \n",
    "    return test_loss, float(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "ANNModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=300, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim).to(device)  # Try use_dropout=False\n",
    "\n",
    "print(f'Device: {device}')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average Loss: 0.003558, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0053, MSE: 0.0224 \n",
      "\n",
      "Train Epoch: 2 Average Loss: 0.001209, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0044, MSE: 0.0272 \n",
      "\n",
      "Train Epoch: 3 Average Loss: 0.001037, elapsed time:0.13s\n",
      "Test set: Average loss: 0.0040, MSE: 0.0449 \n",
      "\n",
      "Train Epoch: 4 Average Loss: 0.000958, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0038, MSE: 0.0160 \n",
      "\n",
      "Train Epoch: 5 Average Loss: 0.000899, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0036, MSE: 0.0128 \n",
      "\n",
      "Train Epoch: 6 Average Loss: 0.000856, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0035, MSE: 0.0345 \n",
      "\n",
      "Train Epoch: 7 Average Loss: 0.000817, elapsed time:0.13s] \tLoss: 0.017332\n",
      "Test set: Average loss: 0.0035, MSE: 0.0158 \n",
      "\n",
      "Train Epoch: 8 Average Loss: 0.000785, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0032, MSE: 0.0093 \n",
      "\n",
      "Train Epoch: 9 Average Loss: 0.000739, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0032, MSE: 0.0347 \n",
      "\n",
      "Train Epoch: 10 Average Loss: 0.000720, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0030, MSE: 0.0382 \n",
      "\n",
      "Train Epoch: 11 Average Loss: 0.000692, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0030, MSE: 0.0262 \n",
      "\n",
      "Train Epoch: 12 Average Loss: 0.000659, elapsed time:0.13s\n",
      "Test set: Average loss: 0.0028, MSE: 0.0370 \n",
      "\n",
      "Train Epoch: 13 Average Loss: 0.000642, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0028, MSE: 0.0242 \n",
      "\n",
      "Train Epoch: 14 Average Loss: 0.000622, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0027, MSE: 0.0109 \n",
      "\n",
      "Train Epoch: 15 Average Loss: 0.000604, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0026, MSE: 0.0208 \n",
      "\n",
      "Train Epoch: 16 Average Loss: 0.000583, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0026, MSE: 0.0257 \n",
      "\n",
      "Train Epoch: 17 Average Loss: 0.000578, elapsed time:0.10s4%)] \tLoss: 0.024707\n",
      "Test set: Average loss: 0.0025, MSE: 0.0097 \n",
      "\n",
      "Train Epoch: 18 Average Loss: 0.000561, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0025, MSE: 0.0145 \n",
      "\n",
      "Train Epoch: 19 Average Loss: 0.000555, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0025, MSE: 0.0120 \n",
      "\n",
      "Train Epoch: 20 Average Loss: 0.000543, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0025, MSE: 0.0275 \n",
      "\n",
      "Train Epoch: 21 Average Loss: 0.000544, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0025, MSE: 0.0109 \n",
      "\n",
      "Train Epoch: 22 Average Loss: 0.000530, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0026, MSE: 0.0135 \n",
      "\n",
      "Train Epoch: 23 Average Loss: 0.000524, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0233 \n",
      "\n",
      "Train Epoch: 24 Average Loss: 0.000518, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0267 \n",
      "\n",
      "Train Epoch: 25 Average Loss: 0.000517, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0134 \n",
      "\n",
      "Train Epoch: 26 Average Loss: 0.000512, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0025, MSE: 0.0248 \n",
      "\n",
      "Train Epoch: 27 Average Loss: 0.000514, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0111 \n",
      "\n",
      "Train Epoch: 28 Average Loss: 0.000507, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0093 \n",
      "\n",
      "Train Epoch: 29 Average Loss: 0.000512, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0101 \n",
      "\n",
      "Train Epoch: 30 Average Loss: 0.000505, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0196 \n",
      "\n",
      "Train Epoch: 31 Average Loss: 0.000503, elapsed time:0.11s36%)] \tLoss: 0.014207\n",
      "Test set: Average loss: 0.0024, MSE: 0.0171 \n",
      "\n",
      "Train Epoch: 32 Average Loss: 0.000494, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0157 \n",
      "\n",
      "Train Epoch: 33 Average Loss: 0.000496, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0213 \n",
      "\n",
      "Train Epoch: 34 Average Loss: 0.000498, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0076 \n",
      "\n",
      "Train Epoch: 35 Average Loss: 0.000491, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0161 \n",
      "\n",
      "Train Epoch: 36 Average Loss: 0.000492, elapsed time:0.13s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0254 \n",
      "\n",
      "Train Epoch: 37 Average Loss: 0.000491, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0131 \n",
      "\n",
      "Train Epoch: 38 Average Loss: 0.000490, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0249 \n",
      "\n",
      "Train Epoch: 39 Average Loss: 0.000487, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0178 \n",
      "\n",
      "Train Epoch: 40 Average Loss: 0.000485, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0210 \n",
      "\n",
      "Train Epoch: 41 Average Loss: 0.000487, elapsed time:0.13s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0088 \n",
      "\n",
      "Train Epoch: 42 Average Loss: 0.000482, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0142 \n",
      "\n",
      "Train Epoch: 43 Average Loss: 0.000490, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0305 \n",
      "\n",
      "Train Epoch: 44 Average Loss: 0.000486, elapsed time:0.15s48%)] \tLoss: 0.019829\n",
      "Test set: Average loss: 0.0023, MSE: 0.0147 \n",
      "\n",
      "Train Epoch: 45 Average Loss: 0.000485, elapsed time:0.24s4%)] \tLoss: 0.014871\n",
      "Test set: Average loss: 0.0023, MSE: 0.0241 \n",
      "\n",
      "Train Epoch: 46 Average Loss: 0.000481, elapsed time:0.22s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0142 \n",
      "\n",
      "Train Epoch: 47 Average Loss: 0.000492, elapsed time:0.14s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0124 \n",
      "\n",
      "Train Epoch: 48 Average Loss: 0.000479, elapsed time:0.16s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0100 \n",
      "\n",
      "Train Epoch: 49 Average Loss: 0.000483, elapsed time:0.14s\n",
      "Test set: Average loss: 0.0024, MSE: 0.0142 \n",
      "\n",
      "Train Epoch: 50 Average Loss: 0.000487, elapsed time:0.16s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0161 \n",
      "\n",
      "Train Epoch: 51 Average Loss: 0.000480, elapsed time:0.16s60%)] \tLoss: 0.019198\n",
      "Test set: Average loss: 0.0023, MSE: 0.0239 \n",
      "\n",
      "Train Epoch: 52 Average Loss: 0.000482, elapsed time:0.17s\n",
      "Epoch 00052: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0431 \n",
      "\n",
      "Train Epoch: 53 Average Loss: 0.000476, elapsed time:0.15s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0105 \n",
      "\n",
      "Train Epoch: 54 Average Loss: 0.000476, elapsed time:0.15s96%)] \tLoss: 0.011660\n",
      "Test set: Average loss: 0.0023, MSE: 0.0300 \n",
      "\n",
      "Train Epoch: 55 Average Loss: 0.000467, elapsed time:0.16s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0213 \n",
      "\n",
      "Train Epoch: 56 Average Loss: 0.000470, elapsed time:0.17s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0163 \n",
      "\n",
      "Train Epoch: 57 Average Loss: 0.000467, elapsed time:0.17s4%)] \tLoss: 0.012358\n",
      "Test set: Average loss: 0.0023, MSE: 0.0251 \n",
      "\n",
      "Train Epoch: 58 Average Loss: 0.000471, elapsed time:0.17s36%)] \tLoss: 0.016055\n",
      "Test set: Average loss: 0.0023, MSE: 0.0100 \n",
      "\n",
      "Train Epoch: 59 Average Loss: 0.000470, elapsed time:0.15s\n",
      "Epoch 00059: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0140 \n",
      "\n",
      "Train Epoch: 60 Average Loss: 0.000466, elapsed time:0.18s48%)] \tLoss: 0.012547\n",
      "Test set: Average loss: 0.0023, MSE: 0.0097 \n",
      "\n",
      "Train Epoch: 61 Average Loss: 0.000464, elapsed time:0.17s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0188 \n",
      "\n",
      "Train Epoch: 62 Average Loss: 0.000464, elapsed time:0.17s48%)] \tLoss: 0.023147\n",
      "Test set: Average loss: 0.0023, MSE: 0.0159 \n",
      "\n",
      "Train Epoch: 63 Average Loss: 0.000465, elapsed time:0.18s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0170 \n",
      "\n",
      "Train Epoch: 64 Average Loss: 0.000465, elapsed time:0.23s48%)] \tLoss: 0.016208\n",
      "Test set: Average loss: 0.0023, MSE: 0.0111 \n",
      "\n",
      "Train Epoch: 65 Average Loss: 0.000464, elapsed time:0.18s\n",
      "Epoch 00065: reducing learning rate of group 0 to 2.7000e-06.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0161 \n",
      "\n",
      "Train Epoch: 66 Average Loss: 0.000463, elapsed time:0.24s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0138 \n",
      "\n",
      "Train Epoch: 67 Average Loss: 0.000463, elapsed time:0.19s36%)] \tLoss: 0.014482\n",
      "Test set: Average loss: 0.0023, MSE: 0.0132 \n",
      "\n",
      "Train Epoch: 68 Average Loss: 0.000463, elapsed time:0.28s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0157 \n",
      "\n",
      "Train Epoch: 69 Average Loss: 0.000463, elapsed time:0.31s] \tLoss: 0.012053 [672/800 (84%)] \tLoss: 0.015738\n",
      "Test set: Average loss: 0.0023, MSE: 0.0152 \n",
      "\n",
      "Train Epoch: 70 Average Loss: 0.000463, elapsed time:0.18s36%)] \tLoss: 0.010818\n",
      "Test set: Average loss: 0.0023, MSE: 0.0399 \n",
      "\n",
      "Train Epoch: 71 Average Loss: 0.000463, elapsed time:0.21s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0135 \n",
      "\n",
      "Train Epoch: 72 Average Loss: 0.000463, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0159 \n",
      "\n",
      "Train Epoch: 73 Average Loss: 0.000463, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0249 \n",
      "\n",
      "Train Epoch: 74 Average Loss: 0.000463, elapsed time:0.10s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0151 \n",
      "\n",
      "Train Epoch: 75 Average Loss: 0.000463, elapsed time:0.09s\n",
      "Epoch 00075: reducing learning rate of group 0 to 8.1000e-07.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0139 \n",
      "\n",
      "Train Epoch: 76 Average Loss: 0.000463, elapsed time:0.12s4%)] \tLoss: 0.008329\n",
      "Test set: Average loss: 0.0023, MSE: 0.0050 \n",
      "\n",
      "Train Epoch: 77 Average Loss: 0.000463, elapsed time:0.14s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0159 \n",
      "\n",
      "Train Epoch: 78 Average Loss: 0.000463, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0120 \n",
      "\n",
      "Train Epoch: 79 Average Loss: 0.000463, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0208 \n",
      "\n",
      "Train Epoch: 80 Average Loss: 0.000462, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0116 \n",
      "\n",
      "Train Epoch: 81 Average Loss: 0.000463, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0173 \n",
      "\n",
      "Train Epoch: 82 Average Loss: 0.000463, elapsed time:0.11s6%)] \tLoss: 0.022392\n",
      "Test set: Average loss: 0.0023, MSE: 0.0147 \n",
      "\n",
      "Train Epoch: 83 Average Loss: 0.000462, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0121 \n",
      "\n",
      "Train Epoch: 84 Average Loss: 0.000463, elapsed time:0.10s\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.4300e-07.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0107 \n",
      "\n",
      "Train Epoch: 85 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0173 \n",
      "\n",
      "Train Epoch: 86 Average Loss: 0.000462, elapsed time:0.13s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0030 \n",
      "\n",
      "Train Epoch: 87 Average Loss: 0.000462, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0337 \n",
      "\n",
      "Train Epoch: 88 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0090 \n",
      "\n",
      "Train Epoch: 89 Average Loss: 0.000462, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0157 \n",
      "\n",
      "Train Epoch: 90 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0178 \n",
      "\n",
      "Train Epoch: 91 Average Loss: 0.000462, elapsed time:0.15s\n",
      "Epoch 00091: reducing learning rate of group 0 to 7.2900e-08.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0114 \n",
      "\n",
      "Train Epoch: 92 Average Loss: 0.000462, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0140 \n",
      "\n",
      "Train Epoch: 93 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0148 \n",
      "\n",
      "Train Epoch: 94 Average Loss: 0.000462, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0297 \n",
      "\n",
      "Train Epoch: 95 Average Loss: 0.000462, elapsed time:0.12s36%)] \tLoss: 0.021541\n",
      "Epoch 00095: reducing learning rate of group 0 to 2.1870e-08.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0163 \n",
      "\n",
      "Train Epoch: 96 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0170 \n",
      "\n",
      "Train Epoch: 97 Average Loss: 0.000462, elapsed time:0.12s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0096 \n",
      "\n",
      "Train Epoch: 98 Average Loss: 0.000462, elapsed time:0.11s48%)] \tLoss: 0.018934\n",
      "Test set: Average loss: 0.0023, MSE: 0.0313 \n",
      "\n",
      "Train Epoch: 99 Average Loss: 0.000462, elapsed time:0.09s\n",
      "Epoch 00099: reducing learning rate of group 0 to 6.5610e-09.\n",
      "Test set: Average loss: 0.0023, MSE: 0.0111 \n",
      "\n",
      "Train Epoch: 100 Average Loss: 0.000462, elapsed time:0.11s\n",
      "Test set: Average loss: 0.0023, MSE: 0.0145 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/model')\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.0001  # will start with big LR to see the lr_scheduler work\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.NAdam(model.parameters(), lr=lr, momentum_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=3, verbose=True)\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_mse = 100000.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_mse = train(model, device, trainloader,  criterion, optimizer, epoch)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    test_loss, test_mse = test(model, device, testloader, criterion)\n",
    "\n",
    "    writer.add_scalars('Loss',\n",
    "                        {\n",
    "                            'train': train_loss,\n",
    "                            'test': test_loss\n",
    "                        },\n",
    "                        epoch)\n",
    "\n",
    "    writer.add_scalars('MSE',\n",
    "                        {\n",
    "                            'train': train_mse,\n",
    "                            'test': test_mse\n",
    "                        },\n",
    "                        epoch)\n",
    "\n",
    "    # deep copy the model\n",
    "    if test_mse < best_mse:\n",
    "        best_mse = test_mse\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "writer.close()\n",
    "# we can also save every X epochs, so if our machine crashes we don't lose our progress\n",
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6859, 0.6289, 0.6624],\n",
       "        [0.6298, 0.6786, 0.6926],\n",
       "        [0.6831, 0.6277, 0.6449],\n",
       "        [0.6896, 0.6354, 0.6454],\n",
       "        [0.5127, 0.4902, 0.5253],\n",
       "        [0.5440, 0.4827, 0.5433],\n",
       "        [0.7748, 0.6618, 0.6998],\n",
       "        [0.6530, 0.7115, 0.7109],\n",
       "        [0.6713, 0.6446, 0.6704],\n",
       "        [0.7883, 0.6826, 0.7107]], grad_fn=<SliceBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>reading score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     math score  writing score  reading score\n",
       "811        0.56           0.56           0.65\n",
       "76         0.47           0.59           0.63\n",
       "636        0.63           0.56           0.56\n",
       "973        0.78           0.67           0.71\n",
       "938        0.45           0.32           0.33\n",
       "899        0.45           0.27           0.31\n",
       "280        0.85           0.80           0.81\n",
       "883        0.70           0.80           0.78\n",
       "761        0.70           0.73           0.79\n",
       "319        0.83           0.79           0.85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "data = MyDataset(X_test.to_numpy(), y_test.to_numpy())\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "model.load_state_dict(best_model_wts)\n",
    "output = model(data.X)\n",
    "display(output[10:20], y_test[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.28436279296875\n"
     ]
    }
   ],
   "source": [
    "print(best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47931605606162014\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, output.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.3, 0.3, 0.3)),\n",
    "    transforms.RandomRotation((-45, 45))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='triple_mnist/test', transform=transform)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='triple_mnist/train', transform=transform)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root='triple_mnist/val', transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset loaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {'train': train_dataset, 'val': val_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset[0][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABaCAYAAAChWQ3bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2SklEQVR4nO2deVyU1f7432c2GBaRXFBAAZHSTMQF3BD3PbTS3LpaectvVr++bbfQUjOzsm5Z1q1u+6ZluOR+1TRTxFRURC3UJDYFBVFB9oHz+2OW2GeAGWDud96v17xm5lnO8znnOc/nOedzPudzhJQSBw4cOHBgfyiaWwAHDhw4cNAwHArcgQMHDuwUhwJ34MCBAzvFocAdOHDgwE5xKHAHDhw4sFMcCtyBAwcO7BS7VeBCCCmEyBdCLG9uWRw4cOBACPGlEKJQCJHeZBeVUtrlB5BA1wr//YHkKv+3A9eATOB9QGXYdx9ws8KnwJBeX8P+4cDPwI2KaVa8toUyVpWpO7DXkO4fwN0V9mmAdUCyQZZhtaSpARKBdCvJdLPKpwx4r4bzlhjkGlVhW2vgK+CK4fOSlWSq9d4Z9j9kKL+bwH8A7wr7dlTJTwlwysb3zt9QNhWvu6jCfifgI+AykANsAXwaK1OF7UFAEfBthW3m6rjNZAJmAL8D+cAFYIiF5WSr+vQtkAHkAueAh6ocPxL9M1WA/rn3a2h9AoZR5dm05adJLmITwc0r8O3Al4Az0AE4BTxRS1oPGCqaMPwPA2YD82p5YOpdkQCVofI8DSiBEYYKfqthvwZ4Egg3VLZhtaT5ArC/aiVpzANXYZ+roZJGVNkeaCi/S1RW4F8A0YCLId0LwIONlamuewcMNTzcPQxl9iHwSx1p7wMW2/je+Rvqo6qWtJ4DTgJehjx9A2yw1r0DdgEHqKDALajjNpEJGA2kAAPQ9/B9MLwYLCgnW9WnHoCT4Xc39I0C44usLfqX8r2GcngT+LWh9YkmVuB2a0KxgADgByllkZQyE31LrUctx94PfC3/uiNHpJTfAElWlKcb4A2slFKWSSn3AgfRvyiQUpZIKd+RUsagbwVXQwgRAPwNeM2KclVkKnrleKDK9veB59G3PioSCbwhpSyQUiYDnwFzrSBHXfcuEoiWUp6RUpYAy4AIIURg1USEEP7AEPTKqTHUee8sIADYKaW8LKUsAr6n9rpYL4QQM4DrwB4zh1aq4zaUaSnwspTyVylluZTyopTyooXn2qQ+GepKsfGv4WOsL/cAZ6SU0YZyeAnoJYToVjUdK9Ynq/Ffo8CllMlSSv8Km94FZgghXIQQPsB49IqgEkIIPyAC+Loe1xINkKmmcwRwh6XXBd4DFgKFVpKpKlUfcoQQ9wIlUsrttZwjqvw25acRMtV170QN14Say3EOcEBK+WcjZbL03qUIIdKFEF8IIdpW2P4ZMFgI4S2EcEFv3tjRSJkQQrQCXgaeqeu8Wuq41WUSQiiBfkA7IcQfhrJ4XwihrXJabeUEtqlPCCE+EEIUoDeVZKDv5YH+pXWywrlGs09NL7MG1ydb8V+jwGvgF/Q3IRdIB+KAH2s4rtpNsRGJ6Fu3/xBCqIUQY9CbA1wsOVkIcTf6rudGWwgnhOhskOerCtvcgFfRm3Zq4j9AlBDCXQjRFX1ryaL8mKGue7cdmCaECDYohsXoW1Q1XXcOelNMYzF377KBUMAP6Au4A6srnH8OSAUuGvLUHb3ibSzLgM+klGlmjqupjttCJi9Ajb4nNwQIAXoDLxr2mysnW9UnpJSPGq43BNgAGFvkbuhNKBW5YTi2KtaqT1bjv1KBCyEUwE70N8oVvZ3LE1hRw+FzqKC0bIWUshS4C5iI3gb3DPADegVVJ0IIV+AN4P/ZUMQ5QEyVh3wp8E0dL7cn0PcGzgObgO+wID91Ye7eSSn3oB9QXY/e1poM5FW9rhAiHL39fF1j5DFcs857J6W8KaWMk1LqpJSXgceBMYYWMujt9M5AG0OeNlChtdsQhBAhwChgpQWH11THrS4Tf/UM35NSZkgps4G3gQlgUTlZvT5VxGD+igF8gfmGzTeBVlUObYW+TpmwZn2yKk1lbLf2hyqDmFX2tTXs96iw7S7gdJXjBqMfjHKvJZ1R1DLgZ6U8xAL/U8P2dCoMYqJvyZSiVx6Z6L0Gygy//a0kyzlgbpVt8ehbTcbrlhmu/XwtabwKfNdIOSy6dxX23Wq4h55Vtn+C3hzUpPfOsM+rYh6A08DkCvtbG/a3bcT1nzTk23hvbqJXfserHFdjHbeFTIZ00oA5Ff5PAU5YUk62qE+1pPsp8K7h9zzgYIV9rui9Ubo1pD7h8EKx+CbUqsAN+5OAKPQeBK2BjcDqKsd8XNNNQd8zcUZve00x/NbUcp2XgH0WyhxsSMsFeBb4E8PouGG/k2F/OjDG8FsY8tChwuce9B4hHQBlY2QyHD+oloe8TZXrpqEfrXcz7A80HKM0lFU20MMK5VTrvTOUyR2GcumM3ivg1Srna9EP7I0wcx2r3DugP3Cbod60AdYCP1c49wv0PQYP9CaGhcDFxshkkKPivfkn+tZhOwvruNVlMhz7MnAUaI++53QAWGZhOVm9PhnkmIHeVKIExhrq+mTD/nboTSZTDPd3BVW8UCytT4Zjh+HwQrEK9wDjgCz0frs64CnjTiGEMzCNms0nEehbM9vRK4lC9K5aNdEJvUeCJcxGP4ByBb3v6Wj51+g4wFnDtXzQmxEK0fuk6qSUmcYP+lZwueF/TR4r9ZEJ9IOXG6SUlbqNUsqrVa5bBlyTUt40HNIXvYtfHnrPmPuklGdquUZ9ZKrr3jkDa9C3OI8Ah4BFVc6/C/1D+bOZ61jr3nVBb7/NQ9+yLQZmVjj3WfR+2ucNeZoA3N0YmaTeU6PivbkJFEkps4zHmKnjVpfJwDL0Cvwcel/wE4Bxsp25crJFfZLozSXp6OcV/BN4Ukq5CcBQXlMMMl5D/5KZUSWNu7CsPjU5Rp9Qu0MIUYS+AqySUlZ9gJtSjnhgpJTyanPJUBWHTJbhkMkyHDJZhhDiM/Q91CtSyq5Nck17VeAOHDhw8H+dRplQhBDjhBBnDT6fUdYSyoEDBw4cmKfBLXCD0/459FNn09HbvWZKKX+znngOHDhw4KA2GtMCDwP+kFImSf2U5u+BydYRy4EDBw4cmKMxCtwHvVuZkXTDNgcOHDhw0ASoGnFuTTEAqtljhBDz0DvLo1ar+7ZtWzX0Qc1U8KtsFApF4zwlrSVHRRorU2MpLy9vdBpCCISwXRiIlipjVbmM6RvriPGadclvDblsUS/B9vfVHPXNl1KpBKCsrLI3bX2fMWvUt5qw1rOekZGRLaVsV3V7YxR4OnpfTCO+6CeXVEJK+TH6yQR4e3vLefPmmU24tLSUnTt3cvr06UaIp6+Mw4YNIzQ0tMEFeebMGbZs2dIoOSqiUqno0aMHKpUKjUZDr169UCqVKBQKXF1drXadmiguLubQoUMcP36ckpKqgQXrh4+PD9OnT0ej0VhJOj3l5eX8+uuvHDhwoNEKKjg4mDFjxqBSNaaa/0ViYiI//vij6b8QgoEDB6LVatm/fz+lpaV4eXnRv39/tm3bVk2pGPH09GTy5Mm0a9euwfXy3LlzbNiwoUHn1kWHDh3w8fmrI+3s7ExwcHAlOZVKJS4uVglRUomkpCQOHTpEWpq50C7652jChAm88MILuLu78+WXX/Lee+9RWFiIUqlkzJgxBAcHW/QyklLy1VdfkZmZaY1smFAqlfTo0QO1Wm3a1tDyXLp0aUpN2xtTs48CQYYQpxfRO7/PakR6Jn7//XeOHz9ulRbG7t27ycrKYsiQIXh4eNT7/LKyMoqLi80faCHFxcUcOXLE9D82NhbQ39hu3bqZKlyPHj1o1UofosHNzc3U0mgoubm5/PTTT5w6dapR6RhJSUnhzJkzhISEWK3FVlhYyIkTJ9i7d2+tyq8+xMXFoVKpGDNmTKPT0ul0xMbGVqsL+fn5TJgwge7du7N79240Gg2RkZHs2LGDgoKCGtPKzMzk888/5/bbbyciIgJPT896y2PtemkkJSWFlJTKuiImJqbSfzc3N4KCgkz/hRD06tULrVYfdNDJyQlnZ2eLr1lSUkJsbCxHjx6ttcyqMmTIEBYvXoy3tzetWrVi9uzZfPHFF1y/fh2AXbt24efnR+vWrc2mpdPpKCwstEl5Hj16tNq2quXp7u5O166V3cYVCgUhISE4Ozvj7l5TXC09DVbgUkqdEOJx9DMGlcDndcyaspjU1FR2795tte5hWVkZx44d4/Lly0ydOrVeSlxKyR9//GEVOWrD2BIuKSmppNjj4uJMb+lbb70VrVaLQqGgd+/epgfE+MDURWlpKcnJyWzZsoW8vDyzx1tKWVkZP/30EwEBARY9JOa4fv0669at49KlS1a791JKEhIS6N69O506dTJ/Qh3s27eP9PTqcZWOHTtGYWEhXbt2JSIigqioKDZu3Gi21V9SUkJ8fDwZGRmEhYUREhLS7Ka12qjaW8vJyeHw4cOVtsXFxZle5IMHD2bo0KEWpZ2cnMzBgwfr9Zy5ubnx1FNPUVBQwPPPP88bb7xBTk5OpXpTUFDA0aNHGTVqlNkGRkpKCleuXLH4+o2lanlevXqVq1erz0c6evQozs7OPPLII7Wm1ai+pdTHiK4tTnS9SU1NJTo6mps3b5o/uJ6kp6ezfv16pk6damrZmkNKafVulaWUlZWZWqFnzvz1Xjx27BgAERERRERE1JlGaWkp27dvJyEhwSY2voKCAjZt2sSUKVNwc3NrcDrXr18nOjqaS5eqWeAaTX5+PtHR0UybNg1fX98GpXHx4kVOnjxZ44ulrKyMU6dOUVZWxsMPP8yVK1d4/fXXTS1Bc1y+fJnt27eTlpbGgAEDaN++vUU9mvPnz9c3GzZFp9OZflvSe8rNzeXYsWMcO3aM/Pz8el2rW7duDBo0iGnTpqHVaikuLsbDw6PadY8ePUqnTp3o1q3a2gyVsJX9u7HodLpK5VoTLeaVX1payq5du2yivI2kpaXx3Xff1fow2gNGxW6u0qWmpvL9998THx9v0wqanJzMkSNHGlSeUkpOnDjB999/bxPlbSQvL4/o6GhycnLqfa5Op2P37t111ks3Nzeee+458vLy+Pvf/17vvJSVlREfH8/nn39OXFwchYXV1uuoRHM2LBqLTqfjwoULfPbZZ+zfv7/eyhugbdu2qFQq7r//fp555hlat25Nu3btqjUiSktLOXTokE11SnPTIhR4aWkp//nPf7h40dKVlxpOZmYm27Zts0iJX7p0idzcXJvLZE1KS0vZu3cv0dHRJCVZc0W42vn111/r/VKUUnLs2DG2b9/O5cuXbSKXQqEwjR3k5uZy6NAhSktL65XGn3/+SWpqap3XmDt3Ls7OzkRFRREfH9/gxkFJSQk7duzghx9+4PLly3bbyKiNGzdusGvXLlavXt2o5+rQoUPExMQQERHBpk2b2LNnDyUlJTWOE6WmpnL06NH/urI0Yp3h+UZgVN7Hjx9v0mtu3663/PTq1avWLmtubq5NBjZsxbVr14iNjSUuLq5Jr2t8aQQGBtY54GKkoKCA06dPs3v3brNdxIaiUqmYP38+AQEBLFmyhLy8PFO5jB071iLPlOvXr7N169Y6H35nZ2cCAwNZuXIlf/7Z+EWdpJQkJyebBjlHjx5tE4+PpkSn05GSksKWLVu4caPq4jf158aNG/zjH/+gqKiI9PR0hg8fjpubG05OTjUef+jQIXx9fSsNvP630OwK/JdffmlS5W3EqMT//PNPxo4da/FD0q5dO0aOHElhYSGDBw+mbdu2LF26tNrIfVNz8eJFfvjhh2brMeTl5bF582ZmzpxZ52DczZs3WbdunU3Ly9PTk+nTp/PKK6/g7u7OqVOn+OKLLwD9YJurqyvDhg2rM43S0lIOHjxotjyLi4tZuXJlna30hmAc5MzMzCQ8PJwePf5aorGoqKjePYnm4vr16xw+fJjDhw9btRVc0cX45MmTjBw5slaX1tLSUmJiYujYsWONYzUVx5jsjWZV4KmpqZw8edL8gTaitLSUhIQEbt68yZQpUyopcSllpUqiVCoJDQ1l+fLlDB48GIVCgRACKSWhoaHMmTOHEydONHkedDod+/fvJz4+3qpeJg0hJSWFhIQEQkJCatyfn5/P+vXrbaq8vb29eeaZZ3jooYfIyspCrVbTs2fPSsccP36crl271jmomZqaahowrouysjKSk5MbK3atZGZmsnnzZv744w8GDBiAl5cX6enpZGdn2+ya1qCsrIyDBw9y7NgxmzcqkpKSEEIwZcoUjh07VuOLIjU1lePHjzNkyJBqPe6aPEDshWazgdvS46S+JCUlsWHDBjIyMiptv3btmul3mzZtWLNmDaGhoWzfvp0lS5bw/fffA+Dq6tosLd/MzEy+/fZbYmJiml15w1+9mpoU2qlTp/jqq69squw6derExx9/zN///ncOHTrEhAkT+O233+jSpUul44yDmjW5BYI+H7/++muLsZsaW+NGs589cOLECX755ZcmeS4uXrzItWvXSE5OrvOeHT9+vEXoG2vSLAo8LS2NdevWtajCvHDhAl9++SW//VZzMEWFQoFGo0GlUuHl5cWECROYOHEiAKtXr7apYqqN06dPk5KS0mIUDeiV3759+yp5F5w6dYqtW7eSlZVVx5m1069fP9q3b1/nMd26deOTTz4hJCSENWvWMHPmTNLS0rh69So9evTA29u70vG5ubmsX7++Ri+I2NhYm/v//7eTm5vbZO55e/fuZdy4cXz99dd1Hnfjxg22bNnSop6XxtLkCtxoW2wJLcaqlJSUmGZO5efnVxrAzMnJ4YMPPuDMmTP4+/szaNAgWrVqxfXr19m0aZNVZg42F1qtlpEjR1ptIonRlGLk6NGjDZq6r1QqmT9/Pq+//jqLFy+uU77i4mJ69uzJE088wbPPPsu1a9coLCwkLS0NhUJR47k3btxg//79lQZS09LSLDKdOKgbJycnxowZ06BZpvXFaAo1534JerfXiqZOe1fmTarApZTs2rWLc+fONeVlG4SxW2akpKSEN998k/Hjx/Pqq69SXFzM+fPnufvuu4mPj28+Qa1A+/btmTVrVr2mP5vj559/bnS5dO7cmcDAQGbOnEl2dnadNuvr16+zatUqzpw5Y5qOrdFoyMjIoGPHjtVa4KCvj0ePHuWnn36irKwMnU7XYhsXFbEHm+29995LZGQkkyZNanQYCGtiHNA03uOLFy826SxMa9PkCvzUqVN2+9YzBit64oknAHjrrbeIjY212BXOxcWlUmCb5kQIYXqwOnbsSJ8+fawamMr4oDRkooaR0aNHc+LECbKzs8nKymLatGm1unxeu3aNFStWcPbsWdO2kpIS8vPzcXZ2Ztq0adxyyy3VzjNOty8uLiYtLY0LFy40WN6moiV4TQghau3ZgN5177nnnmPDhg0trnd67do1tm3bhpSS4uJiu/HoqYkWMZGnMVScrGFNanor9+zZk3fffZeAgAD27dvH+vXr6/UyCg0N5dtvv21QUC1rM3ToUG6//XZAr8CFEBaHGLCUq1evmuzeSqWSzp074+bmZtFUcVdXV4YOHcrp06dxc3MjODiYfv361bvsTp8+jZSSqVOnVoqyV5XCwkI2b95sM7/0loaTkxMqlcr0/CiVSouDkrm5ufHII4+waNEiHn30UdNYUEUuXLhAYWFhi+3NXLhwwSYTB1u3bs3EiRPp3Lmz1dOuiWb3A28MQggeeughwsPDefHFF63miyulrNSSA/2LYvbs2QwcOJB169bx9NNPVzKxmEOj0TBnzhz69u3b7K1wd3d3HnjgAXbt2sWNGzeYNUsfRNLaCtyIQqHgscce4+mnnyY2NpZ58+aZHcD29vYmMDCQ0tJSoqKiuHz5Mm5ubnTo0MHiOCOgV+Cpqak4OzvX+aIvLy+nqKjI4nTtGXd3d9577z3S09PRarW0adMGrVZLQkIC8fHx7Nq1q9ZWaefOnVm0aBHJycl8+OGHFBcX8+yzz3LhwgUSExNrPEehUNCjRw/69+/P/v3762VCVavVuLq6kpeXZ9WWvE6na3RI5YoIIbjtttuYM2cOd999NykpKTz55JO1lom1aPIWeHh4ONOnT2fYsGGMHj2a3r17N3jwzNXVFaVSSd++ffn3v/9d56zKxlJeXs7atWtZuHAhUVFR9Y5FERkZSWRkJCtWrGhQTA5rotFoCAsLMz186enp5Ofn28y3eMCAATz11FP88ccf9OrVy6xHCehfoq6urixZsoSsrCxWrFjBxYsXCQgIqNe109LSiIqKYsWKFbV6GNkT5eXllXp9RlOGEXd3d8aPH19nI6GwsJC4uDjatGlDWloacXFxHDhwgEmTJvHee+/VOZv20UcfJTs7m3/+859cuXKFzp0706dPH0aPHl3rOV26dOG1116jf//+rFy5sppffm34+/sTFRXFrl27GDZsGEIInJyc8Pb2xsfHxypx3q21cMi4ceP48MMPSUpKYty4cRw5coT777/f5hEmm7QFrlKp+Oyzz8jNzUVKSX5+Pp06dWLWrFkcOHDAojQ6duzI2LFjSUtL49dff+Wjjz4iMTGRr776ijfeeINXX32V/fv3N8rOfuPGjRpbiMboafVFo9Ewffp08vLy2LdvX7NHP3N2diYvL4/XX3+dNWvWsGHDBnr27EmvXr1sEiQpKiqKLVu28O233/LBBx9Y1JJKSkpiyZIlaLVadu3aRUFBAd7e3vXukpeVlbF27dqGit7iSE9Pr3SPbrvtNjp37syBAwcoLS1l9uzZtGnThr1799aahk6n44MPPsDJycnkueHr68usWbPYunVrnWV89OhRunTpYjI1Xb16lV9++YWbN2+i0WhMrdqKvZ2JEyeaxiiioqKYPXs2zz33XK3X0Gg0BAYG8tprrzFs2DDWrl2Lr68v8+fPp1evXgQFBeHn58eGDRt4+eWXG2WmsUbYCS8vL6KiosjPz2fNmjUUFBTw9ttvs3btWrp162bThkOTtsCFEBw5coSpU6cyatQoZs+eTWJiImFhYRanoVarmT9/Pp999hmffvopU6ZMITY2lhdffBFfX18+//xzxo0b16iWeHZ2tlViNhhp3bo1PXv2ZMuWLbVOHmlKsrOzWb58OXPnzuWll14iIyMDFxeXBvtp14TRvgr6Vo5x4YqioiKLXq7l5eVs2LCB1atXk5WVhRCCvLy8erfA/9vQ6XSVXoAFBQUIISgsLEStVjNkyBC++OILszF8ysvLTcr7tttuIzo6mlOnTvHee+/VOai3b98+lEol//u//0tYWBhRUVGMGDGCa9euVRo/iIyMJDAwEIVCwa+//oqXlxc9e/bEy8vLrFlh6tSp7N69m8GDB/P1119z8OBBZs+ezUsvvUTv3r3Zu3cvr776KlOmTGl0fbCG2eyRRx4hNDSU7777zpReaWkparXaqp5dNdGkLXBjGNQ1a9Zw7733mlZ12bNnj2mJsZKSkjoHktLS0hg/fjzh4eG89dZbjBw5Ejc3N6Kjo01d7g8//JCJEyfWa7ReoVDg4eFR6zTwhqJQKFi4cCFSSt5++22LfFVtTXFxMZs3bzb9LykpIScnxyoDeEqlkrvuuotBgwaRkJCAQqFAq9Xy7rvvotPpcHV1Zc6cOXz99df1GrOQUqLT6VpcHOyWgFHhlpSUcO7cuXrFZlcqlcyZM4f8/HwWLFhg1rx39epVVq5cycsvv8zGjRtZtmwZCxcuJD8/v9KLWaPR8Oqrr/LRRx+hUqnYtGkTI0eONMWQr4uEhAQyMjL47bffSExMZOXKlZw7d47/+Z//Yf/+/Vy9ehV3d3emT59O9+7dK805aA4SEhK4evUqhw8fNvWuCwsL2bVrF1OmTOHkyZM288SxqAUuhEgWQpwSQsQLIeIM224RQuwWQpw3fJv12C8vL2fVqlXEx8ezYsUK7rvvPnr06MHjjz/Ohx9+yMaNG3nwwQfrTENKSU5ODtu2beP555/n2LFjPP/887zzzjvMmjULNzc3CgsL6/VmdXFxITIyknfeeYeoqCiLz7MEZ2dn+vfvb5oV2BIpLi4mLy+vRje7+qBSqXjwwQeJioqisLCQoUOH0qpVK1auXMmBAwfQarW4ubkRFRXFa6+9Vi9Fo1AouHbtWq0R5/6vIoQw2bvLyspIT083u4BBRQYMGMCsWbP49NNPLR6b8fT0xM/Pj3Xr1jFo0CCg+oSYbdu2ERcXx6OPPsq0adMIDw8nKSmJuXPnmn0Ozpw5w2uvvcbw4cN55ZVXuHDhAo8++igbN240nevj40NAQECL8OhKS0ujvLy8kr27vLyc3bt34+7ublM7eH1a4MOllBVHuaKAPVLK14UQUYb/z9d5MZWKpUuXolarCQ0NZfz48Vy+fJnU1FTTjMZ169bVKYSLiwv9+vUjODiYkJAQunbtipeXFzNmzCAhIYHXX3+d8+fPW7QwqlqtZvz48YwcOZIRI0Zw8OBBFi9eTLt21RZ/bjDdu3cnICCgzpH95kan05GRkUFoaCixsbENGp1XKBTcf//99OnTh4cfftg0c7VNmzZs3LiRb7/9luXLl3Pfffdx5swZQkND0Wq1FodTKCsrY8OGDXa7kIElaDQagoKCGDJkiGkquk6nY9u2bbX23PLy8tBqtSiVSm655RY6dOiAp6enKdBaXWi1WhYsWEBSUhK7du2ySEa1Wk1UVBSnT59m3bp1/Pvf/8bDw6PafczPz+ef//wnzs7OlJeXo1QqKS4utqglKqVk27Zt9O/fnxkzZrBs2bJqQe98fX1Na6g2NxcvXuTmzZvVzCXGgHe2pDEmlMnAMMPvr4B9mFHgoA/AlJyczOrVq1mwYAEPP/wwBw4coLy83OzgnqurK5988gmjR48mNzeXS5cusXXrVn7++WeEEPz888/k5+dbVEm8vLyYNGkSM2fOxMnJiRdeeIGdO3fSsWNHqyrwAQMGoFar+eijj+qlGFUqVbUyEULg6upq9RgypaWlrF69mlGjRjXYjKLVahk+fDiffvop8fHxODs7o9Fo8PHxQUpp8glOTExk9+7djB07tt4vCnuf8VoXCoWCMWPGsGLFCi5cuEB2djbjx4+nVatWvPfee7z11ls1nte1a1c6dOjAvHnzcHNzQ6VSsXHjRrPKu2PHjowcORIfHx8WLVpkceu7S5cudOvWjWeffZapU6eyc+fOWmcyGu97QyguLmb58uVs2rSpkmkC9CGdJ0+ezKZNm2xiPnF1dcXd3Z38/HyLBkgzMzNJTExk8uTJ/Pnnn+Tn5+Pu7s7AgQNtPq/AUgUugV1CCAn8W0r5MeAlpcwAkFJmCCHM+obpdDrmz59PQUEBgYGBzJs3j9TUVIszaVx6avPmzZw9e5a0tDSuX79er0ISQtCnTx9eeuklfH19+eSTT9i2bRupqak2mSG6detWrly5Uq+Wgkql4m9/+xs7duwwrVajVCr529/+xqVLl/jll1/w8/Oz6qzBEydONGr5tdLSUnJzc2nVqhVOTk5MnDiRdu3ambx2hBC0adMGtVrNgw8+yLZt21rsJI/mQKlUcuedd7Jnzx5eeeUVioqKSE5O5rHHHqOwsJDc3NwaF7G+dOkSERERHD9+nJiYGLp3727WNKVWq3nzzTcJCwtj7ty5HDx40GI5XVxc0Gg0jBw5kqSkJLZs2VKtZ+nk5GSVhVByc3M5fPgwXbt2JSsri7y8PFxcXJgzZw6BgYE88MADVl9wxcvLi/fff5+BAwdy+vRp5s+fb3ahjvLyctatW8eTTz7JoEGDuHLlCt7e3igUCu67775q5ePm5oZGo6GsrIw2bdqQmZmJWq2mdevWSCm5dOmSSadpNJo6W/GWKvDBUspLBiW9WwhhsXe6EGIeMA/Aw8ODp59+Gp1Oh0ajITExsV5d4qKiIt58881GKdo2bdrwzDPPEBQUxPLly/n+++9tatpISUmpV/xrIQR33nkno0ePJjo6GtA/cAMHDmT8+PE89thjKBQK2rdvb1UF3ljXxvLycvbu3cvzzz/P1KlT6devH2+//bZpvzG++ty5c02TQJrbnbKloNVq8fPzo3v37mRlZTF06FC8vLwIDw8nOzubH374oVZFlZ6ezltvvWV6JjIzM5k0aVKdCyjodDp27drFpk2biI2NrdfzdPLkSSZMmFDrgrtOTk707NnTaqtCeXp68sknn+Ds7Mzvv/+Oj48PO3fuZP78+TaJYeLk5MTx48fZtm0bYWFhrFmzhtmzZ5uNTrl27VpiY2MZOHAgLi4uJCQkUFBQUC1Etbu7O//617/o3r07hYWFpue4devWBAcHU1payowZM0wmrR49etS52IxFClxKecnwfUUIsREIAy4LIToaWt8dgRpL09Ba/xjAx8dHenp6Eh4eTkJCAp988km9Y2U0tpXs6enJ77//zuLFi7lw4UKLisvi5OREREQEjz76KIsWLSI/P99kW54+fToLFy7k6tWrNndNagg6nY6tW7ei0+kICwvj008/5cSJE9xzzz2mY+Li4jh8+DBvvvmmXQQ0awo6duzIXXfdxfTp0+nWrRv+/v5cvnyZhIQENm3aRE5OjllvnYp1+MaNG6jVarRarSmoV03Hf/PNN9XOtQRLZqxac9AuOzubpUuX0rVrVzw9PYmOjmb79u02e/mnpqby+uuv4+7uTpcuXbj11lvp3LmzWQVeXl5uUWNNpVLh7e1N586dyc/PJycnh4iICG7evIlWq8XJyamSa6Q5G7pZBS6EcAUUUso8w+8xwMvAZuB+4HXDd92+QehNIAsXLqS0tBQpZbMEuTl//jzLly9vka2/zp078/7777Nz504yMjJo3749TzzxBCUlJSxbtszUqgkICGiR3hhFRUX8+OOPbNq0qUbFcOzYMUaPHv1/Jt6IOdRqNQsWLCAoKIgPP/yQJ598kuXLl7Njxw7Ts1FxMNLV1dVsmsXFxeTk5HDHHXdw5MiRWo+zdcNFq9VaNJBqCXv27GHPnj1WS68uhBD07duXZ599lhEjRhATE8OpU6eslv61a9eYO3cut9xyCzdv3uT69ev06dOH3r17s2zZMoqKioiJibE4PUta4F7ARsObQAWskVL+RwhxFPhBCPF3IBW415ILGkfWm5OWqLxBvwL6gw8+yIgRI3j22Wfx9PTkzJkzrFy5slIX+pZbbkGlUhEUFMThw4ebvTyrUttDZvTl/m/B2dmZ1q1bm+J1pKSk1GvQTqvV4u7uTlBQEAsWLOD69eucOXOmUsPGWJa33HIL48ePNzvZSkrZImae9urVi6NHj9YrXpA5mqK33Lt3bz799FMCAgLIycnhH//4h1UnuIG+lV+xV3Xs2DFTvPsTJ05w6dIli9My29eRUiZJKXsZPj2klMsN269KKUdKKYMM380b4MMKaDQa+vTp02zX1+l0HDp0iC1btuDi4kJaWlo15V0RHx8fQkJC6uWqpFAo8PPzs2roWHP06dOn2QN4WRMhBPfffz+LFi3iiy++YNy4caxZs4bFixcTFBRkcXTM3NxclixZwvvvv29aCq62Lni/fv3qjFFSESlls5kGAwMDcXZ2xsXFhcmTJ1slXklTkpiYyEMPPcTUqVM5f/48CxcurHHw2Fo4Ozszc+ZM+vXrx5UrV1i1apXppSeEMDsgbffhZK3JyJEjLQ60Yyt69OjBU089xYsvvsgLL7xQTXmrVCrTRA2VSsWYMWMICgqyKG2tVktYWBibN2/mX//6F23btrW6/DUREhJiNsCSPWHsZs+cORMPDw8SEhJ4/vnnue2229iyZQsLFiywOJxoamoq77zzDh988AEXLlyosXfo5+dHcHCwtbNhEzw8PExK29jAaMkoFAoeeOABk925oKCAuLg4fvrpJ/bt28fo0aPrNeGsvtx2220sXLgQlUrFjh07WL9+vWmfWq2mR48edctvM8lquphCQZ8+fWzu3N4QunTpwh133NGsMmi1WmbOnMlHH31EZmZmja0o45R/I2q1mvDwcLOtsw4dOvD444/zzTff0L17d+655x569+5t9TzURkhISIuMY2JUxpaMKbi6uuLv7095eTnPPfccY8eO5cSJE6xYsYLTp0/zyCOP8NprrzF48GC+/PJLOnXq1Gj51Go1w4YNs8j+3dJQqVRERES0iNmStSGEYOzYsSxYsIAOHToghECr1XLnnXfy8MMPc/nyZauGna1Kz549TeVT02C1OV3Z5C3wESNGMGPGDJvFnm4IXbp0YcqUKXW66zQF7u7uJCQk1Dn4VBOdOnWq0/QTHBzMu+++y9KlSwkICDDFJ7EkrGtDcXJyIiQkhA4dOgD6ihgZGYmfn5/Nrllf3N3diYiIYNiwYRaZPVatWsWqVavQarUUFRVx/vx5VqxYQX5+Pq+88gp5eXl89dVXREZGsmTJEqssGBAREYG/v3+j02ku3N3dmTx5ss3DqtYXJycntFotZWVlphW2fvrpJ3bs2MHOnTv5/PPP8fb2ZvXq1eTm5tpMDqMXXkZGBj/88EO9z2/yUlWpVNx6663ce++9LUKJ+/v7c8899zS78gb9KkBr166tc5C1VatWNdoVBw8eTN++fattF0LwzDPPMGXKFK5du8aqVas4deoU5eXleHh42GQ1ow4dOjBp0iQmT55cyeXRzc2NqVOnNtlqJXXh4eHBjBkzTMrb2dmZe++9ly5dutTY6nFxcaFnz554e3vTpk0b0/bU1FS+/vprPDw8TF1tnU5nml3cWBl79erVqDRaAp06daJLly7NLYaJDh06MHnyZDp27AhAVlYWCxcuZNWqVWRlZXHw4EFefvllxo0bx5dffmnT8YSffvqJI0eOUFpa2qBJSc02wuDr68u9997Lnj17SE5ObhYZjOE37al7GhgYWOOgitGUcu7cuWozHI0hbGNiYnj77bcZM2YMpaWlnD9/3uqunIGBgXW+EN3c3AgPDyc6OrrZYsN4eHgwbdq0SgsdK5VKunTpgp+fHzExMRw/frxSy6uwsJCjR48yduxYxo8fz48//mjyTsjJyaF169ZWXdHHaDqxpf3V2qhUqhrNkCqViqFDh3L58uVmnX2r0WgYN24c3bp1q/YMZWdn8/HHH/Pxxx83qUx5eXksWrSIadOmVZsTU1tjrSLNOkRsDCK/c+fOBi2U0BjUajUTJ05skXbZhtK6dWsiIyP57rvvTK0G48IZoJ+FKoTAxcWFsrIyiwJ+WYpGo2Hs2LF0797d7Kh9165dGTduHP/5z3+aXImHhoYyaNAgWrduXeN+pVLJ0KFDCQoKYu/evSQlJZm8Ol566SVKSkp44YUXuO+++9iwYQPp6enMmDGDnJwcq5oJ/Pz86NmzZ4scL6qNurwmfH196d27N/v3729iqfQEBAQwduxYvLy8muX6dRETE1Oj73eXLl3MPkvN7uOjVqsZO3YsUkqOHz/eZNecMGECwcHBdveAmPMc8ff3p2/fvpWmMsfExJCSkkJaWhp9+vShTZs2XLx40WqDMx07dmTgwIHccccdFpWnEILevXsjpaw0acXWhIaGMmbMGItc27y9vZkxYwYpKSls3ryZ3NxcsrKyWLx4MdHR0QQHB3P33Xdz6623cvLkSZYtW2a1RUDc3d2JjIy0iXmrORkyZAhpaWlmY4tYE41Gw5gxY7j99ttt6g7YXDS7Age9Qh03bhxubm4cP37c6tH2qjJgwACbrp9pK5RKpVmXQbVazZgxY7hy5YppVPvAgQMsWrSIgwcPMmjQIBQKBdnZ2Y0enFGpVPTs2ZPhw4db7KNsRAhBcHAwp0+ftrkJzd3dnT59+hAeHl4vv2SVSkVgYCBz587lwIEDnDhxgry8PGJjYzly5Ajr1q2jbdu2pKWlWXWhjj59+tS7PO0BlUrFyJEjWbt2rc1NKUIIOnXqZAqqZm/PuqW0mKFhtVrN8OHDmTJlik3tfv7+/oSFhdV5Q4UQqFQqu5uEYEStVjN69GjTILGUkjVr1pCSkoK7uztOTk74+Pg0quVrNJlERkY2WNmo1Wruvvtuq7jb1Ya7uztTp05l2LBhDb6fHh4ejB07llmzZhEYGAjoByqzs7NJTEwkPz/farN7/f396devn10qHGO0ybrw8fGxeePJy8uLO++8kxkzZtC+fXu7LEtLaXEayt/fnylTprB+/Xqrt8T9/Py45557zL4gOnfuzGOPPYaUkvj4eAoLCykrKyMxMdEUx6U5poS3bdvW4hmUvr6+jBs3jujoaJM9XAhBq1atKCsrIy4urtZgR3WhUCjw9vYmMjKStm3bNvrhaNWqFVOnTmXt2rX1mkJsadpTpkyxiteLWq0mMDCQTp06ERsbS1xcXL0DsZlDo9EwYMCAOutnxRV4gBa1SEinTp0sMlMMHTq02nRya6BUKunZsydDhw6tdYyjKiqVCrVajU6na1GB7cCyoGAtToGDXolPnTqVgwcPWm0NRK1Wy6RJkyxqLRpj8wIMHz4c0Ldihw0bhpSSkpIS4uPjTS3Y5ORkU0B8o4K3BT4+PvWy4wUFBVWyh0spOXv2LFlZWbRq1QqtVlsv1yWlUsmoUaMICQmxakTEVq1aMWDAgBpjSzcEIQQhISGMGDHC6r05jUbDsGHD8PPz49ChQyQlJVnNhh8aGsqtt95a5zGdOnXi8ccfB/QxfRISEiqZbzIyMqqFMNXpdC0q/o9KpWLw4MGkpaVZ7Vlp3749/fv3JyQkpF6Dyffccw86nY5Tp05VeyFnZmZWa1Q0VVmqVCqLJtq1SAUO+tZyx44d2bZtW6NDjyoUCkaPHo2np9llO2tFCFFJ+Y8aNcr0u6ioyKR4zp49y/Xr1wH4/fffG9TKrY36TkVXqVSEhYWRlJRkkiM2NpaXXnqJmJgYiouLLVbEnp6ehIWFERwcbJNJGXfccQclJSXs2bOn0Q91t27dbD51PyAgAF9fX/7880+2bt3a6BePk5OTRaYTlUpVaf5EREREpf0lJSXVXsp//PFHpXUoc3NzbbI4dH3KOzAwkAEDBlhlSbTu3bsTHh7eoDVdjS7E4eHh1fbVVJZJSUnVgludPXvW6tYClUpl0exg0ZTdBm9vbzlv3rx6naPT6Rq96oZxemxT28IKCwut+rZWq9UNCkJlDTksrVCNwbgEV2PrpJOTU5OOXxQUFDRa5qaso2VlZVb1WTdS3/ppLTm0Wm2zzvQsKiqyuidV1fqwdOnSY1LKftWOa0oFLoTIA8422QVtQ1sg2+xRLRd7lx/sPw/2Lj/Yfx7sTX4/KWW1xXqb2oRytqa3iD0hhIiz5zzYu/xg/3mwd/nB/vNg7/IbaTFuhA4cOHDgoH44FLgDBw4c2ClNrcCbNlKMbbD3PNi7/GD/ebB3+cH+82Dv8gNNPIjpwIEDBw6sh8OE4sCBAwd2SpMpcCHEOCHEWSHEH0KIqKa6bn0QQnwuhLgihDhdYdstQojdQojzhm/PCvsWGPJzVggxtnmk/gshRCchxM9CiN+FEGeEEP9r2G5PeXAWQhwRQpw05GGpYbvd5AFACKEUQpwQQmw1/Lc3+ZOFEKeEEPFCiDjDNnvLQ2shxDohRKLhmRhob3kwizHWsS0/gBK4AHQBNMBJ4PamuHY95YwA+gCnK2x7A4gy/I4CVhh+327IhxMQYMifspnl7wj0Mfx2B84Z5LSnPAjAzfBbDRwGBthTHgxyPQ2sAbbaWz0yyJUMtK2yzd7y8BXwkOG3Bmhtb3kw92mqFngY8IeUMklKWQJ8D0xuomtbjJRyP5BTZfNk9BUBw/ddFbZ/L6UsllL+CfyBPp/NhpQyQ0p53PA7D/gd8MG+8iCllMZ5yWrDR2JHeRBC+AITgU8rbLYb+evAbvIghGiFvkH2GYCUskRKeR07yoMlNJUC9wEqLv+SbthmD3hJKTNAryAB40rALTpPQgh/oDf6Fqxd5cFgfogHrgC7pZT2lod3gOeAivEL7El+0L80dwkhjgkhjPEv7CkPXYAs4AuDKetTIYQr9pUHszSVAq8pwIO9u7+02DwJIdyA9cCTUsq6Vm1okXmQUpZJKUMAXyBMCFF9ocW/aFF5EELcCVyRUlq6RmCLkr8Cg6WUfYDxwGNCiIg6jm2JeVChN4d+KKXsDeSjN5nURkvMg1maSoGnAxWj9vsC1g3+bDsuCyE6Ahi+rxi2t8g8CSHU6JX3ainlBsNmu8qDEUOXdx8wDvvJw2BgkhAiGb2pcIQQ4lvsR34ApJSXDN9XgI3ozQn2lId0IN3QewNYh16h21MezNJUCvwoECSECBBCaIAZwOYmunZj2Qzcb/h9P7CpwvYZQggnIUQAEAQcaQb5TAh96LLPgN+llG9X2GVPeWgnhGht+K0FRgGJ2EkepJQLpJS+Ukp/9PV8r5Tyb9iJ/ABCCFchhLvxNzAGOI0d5UFKmQmkCSFuM2waCfyGHeXBIppwRHgCeq+IC8ALzT16W4uM3wEZQCn6N/LfgTbAHuC84fuWCse/YMjPWWB8C5A/HH23LwGIN3wm2FkegoEThjycBhYbtttNHirINYy/vFDsRn709uOThs8Z4/NqT3kwyBQCxBnq0o+Ap73lwdzHMRPTgQMHDuwUx0xMBw4cOLBTHArcgQMHDuwUhwJ34MCBAzvFocAdOHDgwE5xKHAHDhw4sFMcCtyBAwcO7BSHAnfgwIEDO8WhwB04cODATvn/1NrxkrJ5uiEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.3, 0.3, 0.3])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:8])\n",
    "\n",
    "imshow(out,title=[class_names[x] for x in classes][0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            print(len(dataloaders[phase]))\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.view(-1, 84 * 84 * 3))\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model_ft = ANNModel(84 * 84 * 3, 84, 1000)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\itoml.hw\\Assignment2\\A2test.ipynb Ячейка 46\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "\u001b[1;32md:\\Work\\itoml.hw\\Assignment2\\A2test.ipynb Ячейка 46\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Iterate over data.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(dataloaders[phase]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloaders[phase]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/itoml.hw/Assignment2/A2test.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\lib\\site-packages\\PIL\\Image.py:2962\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2959\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m   2960\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 2962\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[0;32m   2964\u001b[0m preinit()\n\u001b[0;32m   2966\u001b[0m accept_warnings \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ac9f730633c9c3aa54bc61d4a1518eb9b439b7d1460c455a4a24c056f5a2fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
